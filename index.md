---
layout: default
title: Efficiency, Security, and Generalization of Multimedia Foundation Models
description: Workshop @ ACM Multimedia 2024
---

**Welcome to The 1st Workshop on Efficiency, Security, and Generalization of Multimedia Foundation Models at ACM Multimedia 2024!**

---

## **Schedule** {#schedule}

**Location: Meeting Room 216**


|   **Time**  |         **Topic**        |                             **Speakers/Details**                            |
|------------:|:-------------------------|:-----------------------------------------------------------------------------------|
| 14:00 | *Start and Welcome*                                |                                   |
| 14:00-14:30 | Invited Talk                         | Dr Piotr Koniusz (Data61, CSIRO), *"Adversarial Robustness: From Distillation Across Teacher-Student to Few-shot Foundation Models"*                               |
| 14:30-15:00 | Invited Talk                         | Prof Jiebo Luo (University of Rochester)                               |
| 15:00-15:30 | Invited Talk                         | Prof Phoebe Chen (La Trobe University)                               |
| 15:30-16:00 | *Afternoon Tea*                         |                                   |
| 16:00-16:30 | Industry Talk                        | Dr Luoqi Liu (MT Lab, Meitu), *"Driving R&D with User Needs: Putting MiracleVision to Work"*                              |
| 16:30-16:45 | Paper Presentation                   | Object-Driven Human Motion Generation from Images                               |
| 16:45-17:00 | Paper Presentation                   | Rethinking the Role-Play Prompting in Mathematical Reasoning Tasks                               |
| 17:00-17:15 | Paper Presentation                   | ITCD: Image to Text Translation for Classification by Diffusion Models                               |
| 17:15 | *End*                                  |                                   |

---

The rapid progress in foundation models has enhanced the capabilities of multimedia models across a broad spectrum of tasks. Despite their exceptional performance, deploying these models in practical settings raises several concerns, particularly regarding efficiency, security, and generalization. As the utility of foundation models in multimedia topics becomes increasingly evident, addressing these issues is crucial. This workshop focuses on these critical aspects in foundation models, where the scope of the foundation model encompasses a wide range of domains such as vision, language, speech etc., with an emphasis on multimedia tasks and multi-modality methods. 

Therefore, we solicit original research papers in (but not limited to) the following topics:

**Efficiency**
- Efficient network design in foundation models
- Training efficiency of foundation models
- Inference efficiency of foundation models

**Security**
- Adversarial robustness of foundation models
- Privacy and memorization in foundation models
- Trustworthiness and alignment of foundation models

**Generalization**
- Generalization across tasks
- Generalization across data
- Generalization across modalities


---
## **Important Dates** {#dates}

| Submission Open | **May 11, 2024 (AoE)** |
| Submission Deadline | **<strike>July 19, 2024</strike> <span>Extended to July 29 (AoE)</span>** |
| Decision Notification | **August 5, 2024 (AoE)** |
| Camera-Ready Deadline | **August 19, 2024 (AoE)** |
| Workshop Date | **Oct 28 PM, 2024** |

---

## **Invited Speakers** {#speakers}


<div class="container">

<figure>
    <a href="https://www.koniusz.com/">
    <img class="img-author" src="assets/imgs/speakers/piotr_koniusz.jpg" alt="Piotr Koniusz"/></a>
    <b><br><a href="https://www.koniusz.com/">Piotr Koniusz</a>
    <br>Data61, CSIRO</b>
</figure>

<figure>
    <a href="https://www.cs.rochester.edu/u/jluo/">
    <img class="img-author" src="assets/imgs/speakers/jiebo_luo.jpeg" alt="Jiebo Luo"/></a>
    <b><br><a href="https://www.cs.rochester.edu/u/jluo/">Jiebo Luo</a>
    <br>University of Rochester</b>
</figure>


<figure>
    <a href="https://scholars.latrobe.edu.au/ypchen">
    <img class="img-author" src="assets/imgs/speakers/phoebe_chen.jpeg" alt="Phoebe Chen"/></a>
    <b><br><a href="https://scholars.latrobe.edu.au/ypchen">Phoebe Chen</a>
    <br>La Trobe University</b>
</figure>


<figure>
    <a href="https://scholar.google.com/citations?user=nw4XTwMAAAAJ&hl=en">
    <img class="img-author" src="assets/imgs/speakers/luoqi_liu.jpg" alt="Luoqi Liu"/></a>
    <b><br><a href="https://scholar.google.com/citations?user=nw4XTwMAAAAJ&hl=en">Luoqi Liu</a>
    <br>MT Lab, Meitu</b>
</figure>

</div>




<!-- <div class="container">
    <figure>
        <a href="https://beerys.github.io/">
        <img class="img-author" src="assets/imgs/authors/cvpr2024/SaraBeery.jpeg" alt="Sara Beery"/></a>
        <b><br><a href="https://sites.google.com/cs.washington.edu/william-agnew/home">Sara Beery (She/Her)</a>
        <br>Assistant Professor<br>MIT</b>
    </figure>
    <figure>
        <a href="https://sites.google.com/cs.washington.edu/william-agnew/home">
        <img class="img-author" src="assets/imgs/authors/cvpr2024/WilliamAgnew.jpeg" alt="William Agnew"/></a>
        <b><br><a href="https://sites.google.com/cs.washington.edu/william-agnew/home">William Agnew</a>
        <br>CBI Postdoc Fellow<br>CMU</b>
    </figure>
</div>

<div class="bio-text">
<a href="https://beerys.github.io/"><b>Dr. Sara Beery</b></a>
is the Homer A. Burnell Career Development Professor in the MIT Faculty of Artificial Intelligence and Decision-Making. She was previously a visiting researcher at Google, working on large-scale urban forest monitoring as part of the Auto Arborist project. She received her PhD in Computing and Mathematical Sciences at Caltech in 2022, where she was advised by Pietro Perona and awarded the Amori Doctoral Prize for her thesis. Her research focuses on building computer vision methods that enable global-scale environmental and biodiversity monitoring across data modalities, tackling real-world challenges including geospatial and temporal domain shift, learning from imperfect data, fine-grained categories, and long-tailed distributions. She partners with industry, nongovernmental organizations, and government agencies to deploy her methods in the wild worldwide. She works toward increasing the diversity and accessibility of academic research in artificial intelligence through interdisciplinary capacity building and education, and has founded the AI for Conservation slack community, serves as the Biodiversity Community Lead for Climate Change AI, and founded and directs the Summer Workshop on Computer Vision Methods for Ecology.

 -->


<!-- ---

## **Program Committee** {#Committee}

Coming soon -->

---

## **Organizers** {#organizers}
<div class="container">

<figure>
    <a href="https://daochang.site/">
    <img class="img-author" src="assets/imgs/authors/daochang_liu.jpg" alt="Daochang Liu"/></a>
    <b><br><a href="https://daochang.site/">Daochang Liu</a>
    <br>The University of Sydney</b>
</figure>

<figure>
    <a href="https://www.cs.cityu.edu.hk/~minjdong/">
    <img class="img-author" src="assets/imgs/authors/minjing_dong.png" alt="Minjing Dong"/></a>
    <b><br><a href="https://www.cs.cityu.edu.hk/~minjdong/">Minjing Dong</a>
    <br>City University of Hong Kong</b>
</figure>


<figure>
    <a href="https://research.monash.edu/en/persons/yasmeen-george">
    <img class="img-author" src="assets/imgs/authors/yasmeen_george.png" alt="Yasmeen George"/></a>
    <b><br><a href="https://research.monash.edu/en/persons/yasmeen-george">Yasmeen George</a>
    <br>Monash University</b>
</figure>


<figure>
    <a href="http://changxu.xyz/">
    <img class="img-author" src="assets/imgs/authors/chang_xu.jpeg" alt="Chang Xu"/></a>
    <b><br><a href="http://changxu.xyz/">Chang Xu</a>
    <br>The University of Sydney</b>
</figure>

</div>

---
## **Sponsors** {#sponsors}

<figure>
    <a href="https://mtlab.meitu.com/en/?lang=en">
    <img src="assets/imgs/sponsors/meitu.png" alt="Meitu"/>
    </a>
</figure>

---
## **Contact** {#contact}

Contact the organizers at [mm2024-esgmfm@googlegroups.com](mailto:mm2024-esgmfm@googlegroups.com)


## **<strike>Call for Papers</strike>** {#call}

**Submission Deadline: <strike>July 19, 2024</strike> <span>Extended to July 29</span>**

**Submit Platform: [OpenReview](https://openreview.net/group?id=acmmm.org/ACMMM/2024/Workshop/ESGMFM)**

We welcome submissions of research papers, demos, datasets, and position papers within the workshop's scopes.
The submission guideline follows the main conference site of [ACM Multimedia 2024](https://2024.acmmm.org/), including the formatting guideline and submission policies. 
The review process for this workshop will be "double-blinded‚Äù.
Submissions should be of up to 4-page length in [ACM-MM format](https://2024.acmmm.org/files/ACM-MM24-paper-templates.zip), plus up to 1 additional page for the references.

All papers will be peer-reviewed by at least three experts in the field, regarding the relevance to the workshop, scientific novelty, and technical quality. 
Accepted submissions will be presented via oral or poster sessions. 
All accepted papers will be published in the ACM Multimedia proceedings in the ACM Digital Library.

Submit your manuscripts through [OpenReview](https://openreview.net/group?id=acmmm.org/ACMMM/2024/Workshop/ESGMFM).
All the authors need to create a profile on OpenReview. 
New profiles created without an institutional email will go through a moderation process that can take up to two weeks. 
New profiles created with an institutional email will be activated automatically.


<!-- ## Program Committee
## Sponsors -->

